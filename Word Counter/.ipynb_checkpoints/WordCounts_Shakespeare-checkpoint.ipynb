{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark word-frequency counter\n",
    "by Héctor Ramírez\n",
    "\n",
    "What are the most frequent words in the complete works of William Shakespeare?\n",
    "\n",
    "[The complete works were downloaded from http://www.gutenberg.org/ebooks/100 ]\n",
    "\n",
    "### ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import a list of stop words\n",
    "import pandas as pd\n",
    "stop_words = pd.read_csv('stop_words.csv', header=None, index_col=False).iloc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few works in the contents list are:\n",
      "\n",
      "=====================================\n",
      "Contents\n",
      "\n",
      "\n",
      "\n",
      "               THE SONNETS\n",
      "\n",
      "               ALL’S WELL THAT ENDS WELL\n",
      "\n",
      "               THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "\n",
      "               AS YOU LIKE IT\n",
      "\n",
      "               THE COMEDY OF ERRORS\n",
      "\n",
      "               THE TRAGEDY OF CORIOLANUS\n",
      "\n",
      "               CYMBELINE\n",
      "\n",
      "               THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "\n",
      "=====================================\n",
      "\n",
      "Total number of words in the books: 957798\n"
     ]
    }
   ],
   "source": [
    "# To find out where the pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Creating Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"first app\")\n",
    "\n",
    "# Read the Complete works from Shakespeare\n",
    "baseRDD = sc.textFile('Complete_Shakespeare.txt')\n",
    "print('The first few works in the contents list are:\\n')\n",
    "print('=====================================')\n",
    "for line in baseRDD.take(20):\n",
    "    print(line)\n",
    "print('=====================================\\n')\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in the books:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words and their frequencies:\n",
      "\n",
      "('Contents', 20)\n",
      "('SONNETS', 2)\n",
      "('ALL’S', 2)\n",
      "('WELL', 4)\n",
      "('ENDS', 2)\n",
      "('TRAGEDY', 16)\n",
      "('ANTONY', 23)\n",
      "('CLEOPATRA', 7)\n",
      "('LIKE', 2)\n",
      "('COMEDY', 2)\n",
      "\n",
      "\n",
      "The most frequent words:\n",
      "\n",
      "thou has 4514 counts\n",
      "thy has 3918 counts\n",
      "shall has 3246 counts\n",
      "good has 2169 counts\n",
      "would has 2132 counts\n",
      "Enter has 2005 counts\n",
      "thee has 1888 counts\n",
      "hath has 1720 counts\n",
      "like has 1642 counts\n",
      "you, has 1568 counts\n"
     ]
    }
   ],
   "source": [
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "print('The first 10 words and their frequencies:\\n')\n",
    "for word in resultRDD.take(10):\n",
    "print(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "print('The most frequent words:\\n')\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "print(\"{} has {} counts\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping Spark Context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
