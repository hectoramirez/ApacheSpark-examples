{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple PySpark notebook\n",
    "by Héctor Ramírez\n",
    "\n",
    "<hr>\n",
    "\n",
    "This is a simple, instructive notebook on setting up PySpark on Jupyter notebooks/lab and to run it locally.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### First\n",
    "Install pyspark and findspark:\n",
    "\n",
    "* <code> $ pip install pyspark </code>\n",
    "\n",
    "* <code> $ pip install findspark </code>\n",
    "\n",
    "### Second\n",
    "Spark runs on Java 8, then, to avoid issues, it's simpler to only have Java 8:\n",
    "  \n",
    "* Uninstall all Java versions \n",
    "* Install Java 8 from <a href=\"https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\">here</a>.\n",
    "\n",
    "### Third\n",
    "\n",
    "* Add to the <code>~/.bash_profile</code> file the following environments:\n",
    "\n",
    "\n",
    "<code># PySpark</code> <br>\n",
    "<code>export SPARK_HOME=/{YOUR_SPARK_DIRECTORY}/pyspark </code> <br>\n",
    "<code>export PATH=$SPARK_HOME/bin:\\\\$PATH </code> <br>\n",
    "<code># Java</code> <br>\n",
    "<code>if which java > /dev/null; then export JAVA_HOME=\\\\$(/usr/libexec/java_home); fi </code>\n",
    "\n",
    "\n",
    "Remember to replace {YOUR_SPARK_DIRECTORY} with the directory where you unpacked Spark.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Then, we can open a notebook and create a SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.: 5\n",
      "to: 4\n",
      "the: 4\n",
      "Trump: 2\n",
      "had: 2\n",
      "Russians: 2\n",
      "Only: 1\n",
      "one: 1\n",
      "day: 1\n",
      "before: 1\n",
      "spoke: 1\n",
      "Zelensky,: 1\n",
      "Mueller: 1\n",
      "testified: 1\n",
      "Congress: 1\n",
      "about: 1\n",
      "how: 1\n",
      "tried: 1\n",
      "help: 1\n",
      "elect: 1\n",
      "by: 1\n",
      "organizing: 1\n",
      "theft: 1\n",
      "and: 1\n",
      "release: 1\n",
      "of: 1\n",
      "emails: 1\n",
      "damaging: 1\n",
      "his: 1\n",
      "opponent.: 1\n",
      "In: 1\n",
      "that: 1\n",
      "case,: 1\n",
      "were: 1\n",
      "pursuers: 1\n",
      "who: 1\n",
      "sought: 1\n",
      "contacts: 1\n",
      "with: 1\n",
      "Trump’s: 1\n",
      "campaign.: 1\n"
     ]
    }
   ],
   "source": [
    "# To find out where pyspark is\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Creating Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"first app\")\n",
    "\n",
    "# Calculating words count\n",
    "text_file = sc.textFile(\"OneParagraph.txt\") # This is a manually-created file\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b) \\\n",
    "             .map(lambda x: (x[1], x[0])) \\\n",
    "             .sortByKey(ascending=False)\n",
    "\n",
    "# Printing each word with its respective count\n",
    "output = counts.collect()\n",
    "for (count, word) in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "    \n",
    "# Stopping Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
